{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to import GraphFrame\n",
    "##### download and unzip tar spark-1.6.3-bin-hadoop2.6\n",
    "##### export SPARK_HOME=\"/usr/local/bin/spark-1.6.3-bin-hadoop2.6\"\n",
    "##### export PATH=/home/vagrant/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, operator, subprocess\n",
    "from pyspark.sql.types import *\n",
    "from graphframes import *\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd = sc.textFile('data-smaller.csv') \\\n",
    "rdd = sc.textFile('data.csv') \\\n",
    "        .map(lambda line: line.split(',')) \\\n",
    "        .map(lambda elements: tuple([int(elements[i]) for i in range(len(elements))])) \\\n",
    "        .cache()\n",
    "\n",
    "k = 10\n",
    "dimension = 3\n",
    "minPts = k\n",
    "headers = ['age', 'height', 'weight', 'blood_sugar_level', 'child', 'exercise_hours']\n",
    "# max_cluster = rdd.count() / k\n",
    "# min_cluster = rdd.count() / (2*k-1)\n",
    "# loop_for_converge = 20\n",
    "# different_combination = 30\n",
    "eps_range = np.arange(1,10, 0.1)\n",
    "# eps_range = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    return sum([abs(x[i]-y[i]) for i in range(dimension)])\n",
    "\n",
    "def get_nearest_centroid_idx(x, centroids):\n",
    "    dists = {}\n",
    "    for cluster in centroids:\n",
    "        dists[cluster] = dist(x, centroids[cluster])\n",
    "        \n",
    "    cluster = min(dists, key=dists.get)\n",
    "    return cluster\n",
    "\n",
    "def assign_to_cluster(pt, available_centroids):\n",
    "    nearest_centroid = get_nearest_centroid_idx(pt, available_centroids)\n",
    "    return (nearest_centroid, ([pt], [dist(pt, available_centroids[nearest_centroid])]))\n",
    "\n",
    "def calculate_pts_sum(pts):\n",
    "    pts_sum = [0 for _ in range(dimension)]\n",
    "    for pt in pts:\n",
    "        for i in range(dimension):\n",
    "            pts_sum[i] += pt[i]\n",
    "    return pts_sum\n",
    "\n",
    "def write_to_output(assignment, centroids):\n",
    "    tmp = assignment.flatMap(lambda (cluster, pts): [centroids[cluster] for _ in range(len(pts))])\n",
    "    sqlContext.createDataFrame(tmp, headers[:dimension]).save('output.txt', mode='overwrite')\n",
    "    \n",
    "def calc_error(cluster_data):\n",
    "    '''\n",
    "    cluster_data : (cluster_id, [list of row of pts])\n",
    "    '''\n",
    "    #print cluster_data\n",
    "    pts=cluster_data[1]\n",
    "    pts_sum= [0 for _ in range(dimension)]\n",
    "    for pt in pts:\n",
    "        for i in range(dimension):\n",
    "            pts_sum[i]=pts_sum[i]+pt[i]\n",
    "    avg_di = [pts_sum[i]/float(len(cluster_data)) for i in range(dimension)]\n",
    "    error = 0\n",
    "    for pt in pts:\n",
    "        error = error + dist(pt,avg_di)\n",
    "    return (tuple(avg_di), error)\n",
    "\n",
    "def flattenPair(pt,pts):\n",
    "    # print pts\n",
    "    pairs=[]\n",
    "    for neighbor in pts:\n",
    "        pairs += [(pt,neighbor)]\n",
    "    return pairs\n",
    "\n",
    "def assign_nearest(pt):\n",
    "    nearest_cluster = tuple([0 for _ in range(dimension)])\n",
    "    min_error = float('inf')\n",
    "    for centroid in centroidsBC.value:\n",
    "        if dist(pt,centroid)<min_error:\n",
    "            min_error=dist(pt,centroid)\n",
    "            nearest_cluster=centroid\n",
    "    return (pt, nearest_cluster, min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_cost_rdd = None\n",
    "min_cost = float('inf')\n",
    "min_eps = 0\n",
    "\n",
    "eps_records=[] # [eps, number of cluster, number of noise point, error within cluster, error of noise, total error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for eps= 1.0\n",
      "cannot form cluster for this density\n",
      "for eps= 1.1\n",
      "cannot form cluster for this density\n",
      "for eps= 1.2\n",
      "cannot form cluster for this density\n",
      "for eps= 1.3\n",
      "cannot form cluster for this density\n",
      "for eps= 1.4\n",
      "cannot form cluster for this density\n",
      "for eps= 1.5\n",
      "cannot form cluster for this density\n",
      "for eps= 1.6\n",
      "cannot form cluster for this density\n",
      "for eps= 1.7\n",
      "cannot form cluster for this density\n",
      "for eps= 1.8\n",
      "cannot form cluster for this density\n",
      "for eps= 1.9\n",
      "cannot form cluster for this density\n",
      "for eps= 2.0\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.1\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.2\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.3\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.4\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.5\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.6\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.7\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.8\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 2.9\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.0\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.1\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.2\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.3\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.4\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.5\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.6\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.7\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.8\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 3.9\n",
      "noise:  1000\n",
      "number of cluster: 0\n",
      "error within cluster (without noise) 0\n",
      "error of noises:  inf\n",
      "total error:  inf\n",
      "for eps= 4.0\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.1\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.2\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.3\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.4\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.5\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.6\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.7\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.8\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 4.9\n",
      "noise:  827\n",
      "number of cluster: 13\n",
      "error within cluster (without noise) 236604.5\n",
      "error of noises:  512474.0\n",
      "total error:  749078.5\n",
      "for eps= 5.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-aacd685e8315>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetCheckpointDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"checkpoint\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# required for connectedComponents version > 0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnectedComponents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mresultRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow_pt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_pt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# TODO left outer join the original vertic point so that preserve 2 point with same location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/tmp/spark-18dc115e-da04-4750-91d2-e58d30642b29/userFiles-12b0ee5f-c0e2-4d96-934f-13f2ce677311/graphframes_graphframes-0.5.0-spark1.6-s_2.11.jar/graphframes/graphframe.py\u001b[0m in \u001b[0;36mconnectedComponents\u001b[1;34m(self, algorithm, checkpointInterval, broadcastThreshold)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0msetAlgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0msetCheckpointInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpointInterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;33m.\u001b[0m\u001b[0msetBroadcastThreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcastThreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sqlContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    428\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for eps in eps_range:\n",
    "    start_loop_time = datetime.now()\n",
    "    print \"for eps=\", eps\n",
    "    ptsFullNeighborRDD=rdd.cartesian(rdd)\\\n",
    "                            .filter(lambda (pt1,pt2): dist(pt1,pt2)<eps)\\\n",
    "                            .map(lambda (pt1,pt2):(pt1,[pt2]))\\\n",
    "                            .reduceByKey(lambda pts1,pts2: pts1+pts2)\\\n",
    "                            .filter(lambda (pt, pts): len(pts)>=minPts)\n",
    "    edgeRDD=ptsFullNeighborRDD.flatMap(lambda (pt,pts):flattenPair(pt,pts))\n",
    "    vertics = sqlContext.createDataFrame(rdd.map(lambda pt: (pt, \"pt\")),['id','name'])\n",
    "    if (edgeRDD.count()==0):\n",
    "        print \"cannot form cluster for this density\"\n",
    "        continue\n",
    "    edges = sqlContext.createDataFrame(edgeRDD,['src','dst'])\n",
    "    graph = GraphFrame(vertics, edges)\n",
    "    sc.setCheckpointDir(\"checkpoint\") # required for connectedComponents version > 0.3\n",
    "    result = graph.connectedComponents()\n",
    "    resultRDD = result.rdd.map(tuple).map(lambda (row_pt, name, component):(tuple(row_pt),component))\n",
    "    # TODO left outer join the original vertic point so that preserve 2 point with same location\n",
    "    # FullResult = rdd.leftOuterJoin(resultRDD)\n",
    "    groupRDD= resultRDD.map(lambda (id_pt,component):(component,[id_pt])).reduceByKey(lambda pt1,pt2:pt1+pt2)\n",
    "    noiseRDD= groupRDD.filter(lambda (component, pts):len(pts)<k).flatMap(lambda (component, pts):pts).cache()\n",
    "    print \"noise: \",noiseRDD.count()\n",
    "    clusterRDD = groupRDD.filter(lambda (component, pts):len(pts)>=k)\n",
    "    print \"number of cluster:\", clusterRDD.count()\n",
    "    if (clusterRDD.count()==0):\n",
    "        cluster_error = 0\n",
    "    else:\n",
    "        cluster_error = clusterRDD.map(calc_error).map(lambda (c,e):e).reduce(lambda e1,e2:e1+e2)\n",
    "    print \"error within cluster (without noise)\", cluster_error\n",
    "    centroids = clusterRDD.map(calc_error).map(lambda (c,e):c).collect()\n",
    "    centroidsBC = sc.broadcast(centroids)\n",
    "    if (noiseRDD.count() == 0):\n",
    "        noise_error = 0\n",
    "    else:\n",
    "        noise_error =  noiseRDD.map(assign_nearest).map(lambda (pt,nc,e):e).reduce(lambda e1,e2:e1+e2)\n",
    "    print \"error of noises: \", noise_error\n",
    "    total_error = noise_error + cluster_error\n",
    "    print \"total error: \", total_error\n",
    "    \n",
    "    #record time\n",
    "    time_delta = datetime.now() - start_loop_time    \n",
    "    eps_records.append([eps, clusterRDD.count(), noiseRDD.count(), cluster_error, noise_error, total_error, time_delta])\n",
    "    if (total_error<min_cost):\n",
    "        min_eps = eps\n",
    "        min_cost=total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps\tno. of cluster\tno. of noise point\terror within cluster\terror of noise\ttotal error\n",
      "2.0\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:14.616264\t\t\n",
      "2.1\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:09.113957\t\t\n",
      "2.2\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:14.778780\t\t\n",
      "2.3\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:20.075957\t\t\n",
      "2.4\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:25.991672\t\t\n",
      "2.5\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:31.458148\t\t\n",
      "2.6\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:33.946620\t\t\n",
      "2.7\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:34.627438\t\t\n",
      "2.8\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:35.903406\t\t\n",
      "2.9\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:02:42.230598\t\t\n",
      "3.0\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:03:01.063653\t\t\n",
      "3.1\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:03:06.839125\t\t\n",
      "3.2\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:03:09.644126\t\t\n",
      "3.3\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:03:21.430822\t\t\n",
      "3.4\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:03:17.054926\t\t\n",
      "3.5\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:05:29.949627\t\t\n",
      "3.6\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:08:30.411527\t\t\n",
      "3.7\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:08:13.965755\t\t\n",
      "3.8\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:12:27.475028\t\t\n",
      "3.9\t\t0\t\t1000\t\t0\t\tinf\t\tinf\t\t0:11:48.876756\t\t\n",
      "4.0\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t0:21:04.892726\t\t\n",
      "4.1\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t0:24:07.017694\t\t\n",
      "4.2\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t0:22:09.789946\t\t\n",
      "4.3\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t0:29:05.426447\t\t\n",
      "4.4\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t0:40:55.782033\t\t\n",
      "4.5\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t0:56:55.879260\t\t\n",
      "4.6\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t1:06:47.761385\t\t\n",
      "4.7\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t1:06:45.280247\t\t\n",
      "4.8\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t1:11:15.067322\t\t\n",
      "4.9\t\t13\t\t827\t\t236604.5\t\t512474.0\t\t749078.5\t\t1:16:30.461369\t\t\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print \"eps\\tno. of cluster\\tno. of noise point\\terror within cluster\\terror of noise\\ttotal error\"\n",
    "for record in eps_records:\n",
    "    line = \"\"\n",
    "    for number in record:\n",
    "        line =line+ str(number) + \"\\t\\t\"\n",
    "    print line\n",
    "print min_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
