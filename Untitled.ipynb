{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get sample\n",
      "trying to converage: nb_cluster: 400\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5421.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/vagrant/sorted_dist.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1053)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:954)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:863)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1290)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:497)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-338e1e8e3c33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m                                               \u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0madd_pt_to_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                                               lambda (pts1,dists1),(pts2,dists2): add_pt_to_cluster(pts1+pts2, dists1+dists2))\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0msorted_dist_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sorted_dist.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;31m# assign k points to each cluster (achieve k-anonymity)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mpts_to_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1429\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1431\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5421.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/vagrant/sorted_dist.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1053)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:954)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:863)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1290)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:497)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "import random, operator\n",
    "\n",
    "rdd = sc.textFile('data.csv') \\\n",
    "        .map(lambda line: line.split(',')) \\\n",
    "        .map(lambda elements: tuple([int(elements[i]) for i in range(len(elements))])) \\\n",
    "        .cache()\n",
    "\n",
    "k = 10\n",
    "dimension = 3\n",
    "max_cluster = rdd.count() / k\n",
    "loop_for_each_trial = 20\n",
    "\n",
    "def dist(x, y):\n",
    "    return sum([abs(x[i]-y[i]) for i in range(dimension)])\n",
    "\n",
    "def get_dist_from_centroid(x, centroids):\n",
    "    for i in xrange(len(centroids)):\n",
    "        yield (i, (x, dist(x, centroids[i])))\n",
    "\n",
    "def get_nearest_centroid_idx(x):\n",
    "    dists = []\n",
    "    for i in xrange(len(centroids)):\n",
    "        dists.append(dist(x, centroids[i]))\n",
    "        \n",
    "    min_idx, min_val = min(enumerate(dists), key=operator.itemgetter(1))\n",
    "    return min_idx\n",
    "\n",
    "def calculate_centroid(pts_sum, nb_pts):\n",
    "    return [float(pts_sum[i])/nb_pts for i in range(dimension)]\n",
    "\n",
    "def calculate_pt_sum(x, y):\n",
    "    return [x[i]+y[i] for i in range(dimension)]\n",
    "\n",
    "def add_pt_to_cluster(pts, dists):\n",
    "    if(len(pts) > k):\n",
    "        sorted_idx = sorted(range(len(dists)), key=lambda k: dists[k])\n",
    "        return ([pts[i] for i in sorted_idx], [dists[i] for i in sorted_idx])\n",
    "    return (pts, dists)\n",
    "\n",
    "def select_nearest_pts(pts, used_pts):\n",
    "    i = 0\n",
    "    cluster_pts = []\n",
    "    for pt in pts:\n",
    "        if(pt not in cluster_pts):\n",
    "            cluster_pts.append(pt)\n",
    "            if(len(cluster_pts) >= k): break\n",
    "    return cluster_pts\n",
    "\n",
    "# def is_converge(old_cens, new_cens):\n",
    "#     diff = 0\n",
    "#     old_sum = 0\n",
    "#     for i in range(len(old_cens)):\n",
    "#         old_cen = old_cens[i]\n",
    "#         new_cen = new_cens[i]\n",
    "#         for j in range(dimension):\n",
    "#             diff += new_cen[j] - old_cen[j]\n",
    "#             old_sum += old_cen[j]\n",
    "#     return abs(float(diff) / old_sum)\n",
    "\n",
    "min_cost_rdd = None\n",
    "min_cost = float('inf')\n",
    "for want_cluster in range(400, 401):#for nb_cluster in range(1, max_cluster):\n",
    "    print \"trying to get sample\"\n",
    "    for _ in range(30): # try different combination of initial centroid\n",
    "        centroids = rdd.takeSample(False, want_cluster)\n",
    "        nb_cluster = len(centroids)\n",
    "        print \"trying to converage: nb_cluster: \"+ str(nb_cluster)\n",
    "        for _ in range(30):#for _ in range(loop_for_each_trial):\n",
    "            # calculate distances from a point to all centroids, then sort all distances within a cluster\n",
    "            dist_to_cluster_rdd = rdd.flatMap(lambda x: get_dist_from_centroid(x, centroids)).cache()\n",
    "            sorted_dist_rdd = dist_to_cluster_rdd.aggregateByKey(([], []),\n",
    "                                              lambda (pts, dists),(pt, dist): add_pt_to_cluster(pts+[pt], dists+[dist]),\n",
    "                                              lambda (pts1,dists1),(pts2,dists2): add_pt_to_cluster(pts1+pts2, dists1+dists2))\n",
    "            sorted_dist_rdd.saveAsTextFile('sorted_dist.txt')\n",
    "            # assign k points to each cluster (achieve k-anonymity)\n",
    "            pts_to_cluster = {}\n",
    "            for (cluster, (pts, dists)) in sorted_dist_rdd.collect():\n",
    "                for pt in select_nearest_pts(pts, pts_to_cluster):\n",
    "                    pts_to_cluster[pt] = cluster\n",
    "\n",
    "            # assign all remaining points to their nearest cluster\n",
    "            pts_in_cluster_rdd = rdd.map(lambda x: \n",
    "                                         (get_nearest_centroid_idx(x) if x not in pts_to_cluster else pts_to_cluster[x], x)) \\\n",
    "                                .cache()\n",
    "            print \"Before: \" + str(pts_in_cluster_rdd.count())\n",
    "            print \"Has cluster: \"+str(pts_in_cluster_rdd.keys().distinct().count())\n",
    "\n",
    "            # calculate new centroids based on all the point inside a cluster\n",
    "            new_centroids = pts_in_cluster_rdd.aggregateByKey(([0 for _ in range(dimension)], 0), \n",
    "                                    lambda (running_sum, running_count), pt: (calculate_pt_sum(running_sum, pt), running_count+1),\n",
    "                                    lambda (running_sum1, running_count1),(running_sum2, running_count2): \n",
    "                                                          (calculate_pt_sum(running_sum1,running_sum2), running_count1+running_count2))\\\n",
    "                    .mapValues(lambda (pts_sum, nb_pts): calculate_centroid(pts_sum, nb_pts))\\\n",
    "                    .collectAsMap().values()\n",
    "            print \"New centroids size: \" + str(len(new_centroids))\n",
    "    #         print is_converage(centroids, new_centroids)\n",
    "            centroids = new_centroids\n",
    "            cost = pts_in_cluster_rdd.map(lambda (cluster, pt): dist(pt, centroids[cluster])) \\\n",
    "                                     .reduce(lambda x, y: x+y)\n",
    "            print \"Cost: \" + str(cost)\n",
    "            print \"Min cost: \" + str(min_cost)\n",
    "            if(cost < min_cost):\n",
    "                min_cost_rdd = pts_in_cluster_rdd\n",
    "                min_cost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
