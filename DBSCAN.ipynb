{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to import GraphFrame\n",
    "##### download and unzip tar spark-1.6.3-bin-hadoop2.6\n",
    "##### export SPARK_HOME=\"/usr/local/bin/spark-1.6.3-bin-hadoop2.6\"\n",
    "##### export PATH=/home/vagrant/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, operator, subprocess\n",
    "from pyspark.sql.types import *\n",
    "# from pyspark import SparkContext\n",
    "# sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data-smaller.csv') \\\n",
    "        .map(lambda line: line.split(',')) \\\n",
    "        .map(lambda elements: tuple([int(elements[i]) for i in range(len(elements))])) \\\n",
    "        .cache()\n",
    "\n",
    "k = 10\n",
    "dimension = 3\n",
    "headers = ['age', 'height', 'weight', 'blood_sugar_level', 'child', 'exercise_hours']\n",
    "# max_cluster = rdd.count() / k\n",
    "# min_cluster = rdd.count() / (2*k-1)\n",
    "loop_for_converge = 20\n",
    "different_combination = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    return sum([abs(x[i]-y[i]) for i in range(dimension)])\n",
    "\n",
    "def get_nearest_centroid_idx(x, centroids):\n",
    "    dists = {}\n",
    "    for cluster in centroids:\n",
    "        dists[cluster] = dist(x, centroids[cluster])\n",
    "        \n",
    "    cluster = min(dists, key=dists.get)\n",
    "    return cluster\n",
    "\n",
    "def assign_to_cluster(pt, available_centroids):\n",
    "    nearest_centroid = get_nearest_centroid_idx(pt, available_centroids)\n",
    "    return (nearest_centroid, ([pt], [dist(pt, available_centroids[nearest_centroid])]))\n",
    "\n",
    "def calculate_pts_sum(pts):\n",
    "    pts_sum = [0 for _ in range(dimension)]\n",
    "    for pt in pts:\n",
    "        for i in range(dimension):\n",
    "            pts_sum[i] += pt[i]\n",
    "    return pts_sum\n",
    "\n",
    "def calculate_centroid(pts_sum, nb_pts):\n",
    "    nb_pts = float(nb_pts)\n",
    "    return [pts_sum[i]/nb_pts for i in range(dimension)]\n",
    "\n",
    "def calculate_cost(pts, centroid):\n",
    "    cost = 0\n",
    "    for pt in pts:\n",
    "        cost += dist(pt, centroid)\n",
    "    return cost\n",
    "\n",
    "def is_converge(old_cens, new_cens):\n",
    "    diff = 0\n",
    "    old_sum = 0\n",
    "    for i in range(dimension):\n",
    "        old_cen = old_cens[i]\n",
    "        new_cen = new_cens[i]\n",
    "        for j in range(dimension):\n",
    "            diff += abs(new_cen[j] - old_cen[j])\n",
    "            old_sum += old_cen[j]\n",
    "    return abs(float(diff) / old_sum) < 0.000001\n",
    "\n",
    "def write_to_output(assignment, centroids):\n",
    "    tmp = assignment.flatMap(lambda (cluster, pts): [centroids[cluster] for _ in range(len(pts))])\n",
    "    sqlContext.createDataFrame(tmp, headers[:dimension]).save('output.txt', mode='overwrite')\n",
    "    \n",
    "def calc_error(cluster_data):\n",
    "    '''\n",
    "    cluster_data : (cluster_id, [list of row of pts])\n",
    "    '''\n",
    "    #print cluster_data\n",
    "    pts=cluster_data[1]\n",
    "    pts_sum= [0 for _ in range(dimension)]\n",
    "    for pt in pts:\n",
    "        for i in range(dimension):\n",
    "            pts_sum[i]=pts_sum[i]+pt[i]\n",
    "    avg_di = [pts_sum[i]/float(len(cluster_data)) for i in range(dimension)]\n",
    "    error = 0\n",
    "    for pt in pts:\n",
    "        error = error + dist(pt,avg_di)\n",
    "    return (avg_di, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_cost_rdd = None\n",
    "min_cost = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 17\n",
    "minPts = k\n",
    "visited = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def fromCluster(pt1):\n",
    "    print \"from cluster\"\n",
    "    clusterSet=[]\n",
    "    print pt1\n",
    "    if (pt1 not in visited):\n",
    "        if (pt1 in ptsFullNeighborDict):\n",
    "            visited += pt1\n",
    "            queue = deque(ptsFullNeighborDict[pt1]) # ptsFullNeighborDict include itself #still a list\n",
    "            print queue\n",
    "            while (not queue.empty()):\n",
    "                nextpt = queue.dequeue()\n",
    "                if (nextpt in visited): \n",
    "                    # TODO:\n",
    "                    # need to merge cluster since it is parallel\n",
    "                    # may fail to achieve k-anonymity if don't merge\n",
    "                    # neighbor of neighbor -> can ignore\n",
    "                    pass\n",
    "                else:\n",
    "                    visited += nextpt\n",
    "                    clusterSet += nextpt\n",
    "                    if (nextpt in ptsFullNeighborDict):\n",
    "                        for nn_pt in ptsFullNeighborDict[nextpt]:\n",
    "                            queue.push(nn_pt)\n",
    "    return clusterSet # still not mark as visit, may be visited by other point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((43, 183, 1), [(40, 179, 3), (35, 181, 1), (37, 175, 3), (32, 188, 1), (48, 178, 3), (41, 178, 3), (53, 189, 1), (34, 180, 3), (38, 185, 1), (43, 183, 1), (47, 189, 3)]), ((74, 167, 3), [(74, 177, 4), (68, 161, 2), (71, 166, 4), (66, 170, 3), (74, 180, 1), (87, 166, 5), (74, 162, 4), (74, 157, 2), (74, 167, 3), (68, 166, 5)]), ((74, 157, 2), [(68, 161, 2), (71, 166, 4), (76, 149, 4), (77, 147, 1), (74, 162, 4), (74, 157, 2), (66, 153, 4), (75, 150, 5), (74, 167, 3), (82, 153, 3)])]\n"
     ]
    }
   ],
   "source": [
    "ptsFullNeighborRDD=rdd.cartesian(rdd)\\\n",
    "                        .filter(lambda (pt1,pt2): dist(pt1,pt2)<eps)\\\n",
    "                        .map(lambda (pt1,pt2):(pt1,[pt2]))\\\n",
    "                        .reduceByKey(lambda pts1,pts2: pts1+pts2)\\\n",
    "                        .filter(lambda (pt, pts): len(pts)>=minPts)\n",
    "print ptsFullNeighborRDD.take(3)\n",
    "\n",
    "\n",
    "def flattenPair(pt,pts):\n",
    "    # print pts\n",
    "    pairs=[]\n",
    "    for neighbor in pts:\n",
    "        pairs += [(pt,neighbor)]\n",
    "        print pairs\n",
    "    return pairs\n",
    "\n",
    "edgeRDD=ptsFullNeighborRDD.flatMap(lambda (pt,pts):flattenPair(pt,pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=Row(_1=29, _2=151, _3=3), name=u'pt', component=85899345920L), Row(id=Row(_1=40, _2=179, _3=3), name=u'pt', component=85899345920L), Row(id=Row(_1=17, _2=147, _3=5), name=u'pt', component=85899345920L)]\n"
     ]
    }
   ],
   "source": [
    "from graphframes import *\n",
    "vertics = sqlContext.createDataFrame(rdd.map(lambda pt: (pt, \"pt\")),['id','name'])\n",
    "edges = sqlContext.createDataFrame(edgeRDD,['src','dst'])\n",
    "graph = GraphFrame(vertics, edges)\n",
    "sc.setCheckpointDir(\"checkpoint\") # required for connectedComponents version > 0.3\n",
    "result = graph.connectedComponents()\n",
    "print result.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29, 151, 3), (40, 179, 3), (17, 147, 5)]\n",
      "noise:  9 \n",
      "[(12, 190, 5), (53, 149, 4), (98, 178, 2)]\n",
      "number of cluster: 2\n",
      "example of clusters: [(25769803776L, [(86, 140, 4), (79, 183, 4), (55, 149, 4), (100, 142, 3), (74, 177, 4), (90, 140, 5), (68, 161, 2), (88, 161, 5), (71, 166, 4), (83, 184, 2), (76, 149, 4), (83, 183, 2), (68, 178, 2), (66, 170, 3), (74, 180, 1), (88, 185, 3), (87, 166, 5), (77, 147, 1), (66, 189, 2), (58, 153, 5), (57, 178, 2), (61, 171, 2), (95, 153, 4), (74, 162, 4), (85, 175, 3), (89, 171, 2), (97, 140, 1), (93, 142, 2), (74, 157, 2), (66, 153, 4), (89, 155, 4), (75, 150, 5), (85, 182, 2), (54, 154, 5), (74, 167, 3), (100, 143, 1), (68, 166, 5), (94, 142, 5), (93, 173, 5), (91, 179, 3), (57, 178, 4), (82, 147, 1), (81, 141, 2), (64, 190, 2), (96, 150, 4), (80, 142, 2), (82, 153, 3), (54, 152, 4)]), (85899345920L, [(29, 151, 3), (40, 179, 3), (17, 147, 5), (13, 178, 4), (27, 154, 3), (23, 143, 3), (17, 156, 5), (25, 158, 5), (24, 184, 5), (13, 152, 1), (19, 161, 4), (24, 174, 2), (13, 154, 2), (35, 181, 1), (17, 182, 1), (37, 175, 3), (14, 179, 1), (19, 170, 5), (21, 173, 1), (23, 162, 5), (31, 190, 5), (24, 166, 5), (32, 188, 1), (17, 188, 3), (48, 178, 3), (21, 143, 5), (41, 178, 3), (53, 189, 1), (34, 180, 3), (22, 182, 4), (22, 149, 4), (38, 185, 1), (16, 181, 5), (43, 183, 1), (29, 168, 3), (47, 189, 3)])]\n"
     ]
    }
   ],
   "source": [
    "print rdd.take(3)\n",
    "resultRDD = result.rdd.map(tuple).map(lambda (row_pt, name, component):(tuple(row_pt),component))\n",
    "# TODO left outer join the original vertic point so that preserve 2 point with same location\n",
    "# FullResult = rdd.leftOuterJoin(resultRDD)\n",
    "groupRDD= resultRDD.map(lambda (id_pt,component):(component,[id_pt])).reduceByKey(lambda pt1,pt2:pt1+pt2)\n",
    "noiseRDD= groupRDD.filter(lambda (component, pts):len(pts)<k).flatMap(lambda (component, pts):pts).cache()\n",
    "print \"noise: \",noiseRDD.count(), \"\\n\", noiseRDD.take(3)\n",
    "clusterRDD = groupRDD.filter(lambda (component, pts):len(pts)>=k)\n",
    "print \"number of cluster:\", clusterRDD.count()\n",
    "print \"example of clusters:\", clusterRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1873.5, 3884.5, 75.5], 268341.0), ([484.0, 3075.0, 56.0], 122910.0)]\n",
      "error without noise 391251.0\n"
     ]
    }
   ],
   "source": [
    "print clusterRDD.map(calc_error).take(3)\n",
    "print \"error without noise\", clusterRDD.map(calc_error).map(lambda (c,e):e).reduce(lambda e1,e2:e1+e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
