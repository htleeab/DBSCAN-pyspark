{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, operator, subprocess\n",
    "from pyspark.sql.types import *\n",
    "# from pyspark import SparkContext\n",
    "# sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data.csv') \\\n",
    "        .map(lambda line: line.split(',')) \\\n",
    "        .map(lambda elements: tuple([int(elements[i]) for i in range(len(elements))])) \\\n",
    "        .cache()\n",
    "\n",
    "k = 10\n",
    "dimension = 3\n",
    "headers = ['age', 'height', 'weight', 'blood_sugar_level', 'child', 'exercise_hours']\n",
    "# max_cluster = rdd.count() / k\n",
    "# min_cluster = rdd.count() / (2*k-1)\n",
    "loop_for_converge = 20\n",
    "different_combination = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    return sum([abs(x[i]-y[i]) for i in range(dimension)])\n",
    "\n",
    "def get_nearest_centroid_idx(x, centroids):\n",
    "    dists = {}\n",
    "    for cluster in centroids:\n",
    "        dists[cluster] = dist(x, centroids[cluster])\n",
    "        \n",
    "    cluster = min(dists, key=dists.get)\n",
    "    return cluster\n",
    "\n",
    "def assign_to_cluster(pt, available_centroids):\n",
    "    nearest_centroid = get_nearest_centroid_idx(pt, available_centroids)\n",
    "    return (nearest_centroid, ([pt], [dist(pt, available_centroids[nearest_centroid])]))\n",
    "\n",
    "def calculate_pts_sum(pts):\n",
    "    pts_sum = [0 for _ in range(dimension)]\n",
    "    for pt in pts:\n",
    "        for i in range(dimension):\n",
    "            pts_sum[i] += pt[i]\n",
    "    return pts_sum\n",
    "\n",
    "def calculate_centroid(pts_sum, nb_pts):\n",
    "    nb_pts = float(nb_pts)\n",
    "    return [pts_sum[i]/nb_pts for i in range(dimension)]\n",
    "\n",
    "def calculate_cost(pts, centroid):\n",
    "    cost = 0\n",
    "    for pt in pts:\n",
    "        cost += dist(pt, centroid)\n",
    "    return cost\n",
    "\n",
    "def is_converge(old_cens, new_cens):\n",
    "    diff = 0\n",
    "    old_sum = 0\n",
    "    for i in range(dimension):\n",
    "        old_cen = old_cens[i]\n",
    "        new_cen = new_cens[i]\n",
    "        for j in range(dimension):\n",
    "            diff += abs(new_cen[j] - old_cen[j])\n",
    "            old_sum += old_cen[j]\n",
    "    return abs(float(diff) / old_sum) < 0.000001\n",
    "\n",
    "def write_to_output(assignment, centroids):\n",
    "    tmp = assignment.flatMap(lambda (cluster, pts): [centroids[cluster] for _ in range(len(pts))])\n",
    "    sqlContext.createDataFrame(tmp, headers[:dimension]).save('output.txt', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_cost_rdd = None\n",
    "min_cost = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 2\n",
    "minPts = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smaller(pt1,pt2):\n",
    "    for i in range(dimension):\n",
    "        if pt1[i]<pt2[i]:\n",
    "            return True\n",
    "    if pt1==pt2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def fromCluster(pt1):\n",
    "    clusterSet=[]\n",
    "    if (pt1 not in visited):\n",
    "        if (pt1 in ptsFullNeighborDict):\n",
    "            visited += pt1\n",
    "            queue = deque(ptsFullNeighborDict[pt1]) # ptsFullNeighborDict include itself #still a list\n",
    "            while (not queue.empty()):\n",
    "                nextpt = queue.dequeue()\n",
    "                if (nextpt in visited): \n",
    "                    # TODO:\n",
    "                    # need to merge cluster since it is parallel\n",
    "                    # may fail to achieve k-anonymity if don't merge\n",
    "                    # neighbor of neighbor -> can ignore\n",
    "                    pass\n",
    "                else:\n",
    "                    visited += nextpt\n",
    "                    clusterSet += nextpt\n",
    "                    if (nextpt in ptsFullNeighborDict):\n",
    "                        for nn_pt in ptsFullNeighborDict[nextpt]:\n",
    "                            queue.push(nn_pt)\n",
    "    return clusterSet # still not mark as visit, may be visited by other point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add label\n",
    "rdd.take(3)\n",
    "ptsFullNeighborRDD=rdd.cartesian(rdd)\\\n",
    "                        .filter(lambda (pt1,pt2): dist(pt1,pt2)<eps)\\\n",
    "                        .map(lambda (pt1,pt2):(pt1,[pt2]))\\\n",
    "                        .filter(lambda (pt, pts): len(pts)>=minPts)\\\n",
    "                        .reduceByKey(lambda pts1,pts2: pts1+pts2)#(pt1,[pt2,pt3])\n",
    "ptsFullNeighborDict = ptsLargerNeighborRDD.collectAsMap()\n",
    "\n",
    "ptsLargerNeighborRDD=rdd.cartesian(rdd)\\\n",
    "                        .filter(lambda (pt1,pt2): smaller(pt1,pt2))\\\n",
    "                        .filter(lambda (pt1,pt2): dist(pt1,pt2)<eps)\\\n",
    "                        .map(lambda (pt1,pt2):(pt1,[pt2]))\\\n",
    "                        .reduceByKey(lambda pts1,pts2: pts1+pts2)\\\n",
    "                        .cache()#(pt1,[pt2,pt3])\n",
    "ptsLargerNeighborDict = ptsLargerNeighborRDD.collectAsMap()\n",
    "#neighbor include itself\n",
    "#different pt at same location(have same content) can be ignored (since its largerNeighbor must same, it is covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((24, 184, 5), [(24, 184, 5)]), ((94, 176, 1), [(94, 176, 1), (94, 176, 1), (94, 176, 2), (94, 176, 1), (94, 176, 1), (94, 176, 2)]), ((48, 174, 2), [(48, 174, 2)])]\n",
      "[((24, 184, 5), [(24, 184, 5)]), ((94, 176, 1), [(94, 176, 1), (94, 176, 1), (94, 176, 2), (94, 176, 1), (94, 176, 1), (94, 176, 2)]), ((48, 174, 2), [(48, 174, 2)])]\n"
     ]
    }
   ],
   "source": [
    "print ptsFullNeighborRDD.take(3)\n",
    "print ptsLargerNeighborRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1220, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-16-c9e2244838c4>\", line 4, in fromCluster\nUnboundLocalError: local variable 'visited' referenced before assignment\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cf8dc31b0101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvisited\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfromCluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# rdd.map(fromCluster).filter(lambda clusterSet: not clusterSet).take(3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,\n\u001b[1;32m--> 842\u001b[1;33m                                           allowLocal)\n\u001b[0m\u001b[0;32m    843\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1220, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-16-c9e2244838c4>\", line 4, in fromCluster\nUnboundLocalError: local variable 'visited' referenced before assignment\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "visited = []\n",
    "rdd.map(fromCluster).take(3)\n",
    "# rdd.map(fromCluster).filter(lambda clusterSet: not clusterSet).take(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
