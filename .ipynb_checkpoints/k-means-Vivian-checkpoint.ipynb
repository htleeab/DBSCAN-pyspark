{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying 526 clusters\n",
      "trying combination 0/30\n",
      "trying to converge 0/20\n",
      "cost: 223139.460424\n",
      "trying to converge 1/20\n",
      "cost: 260233.705755\n",
      "trying to converge 2/20\n",
      "cost: 283736.998457\n",
      "trying to converge 3/20\n",
      "cost: 279588.768123\n",
      "trying to converge 4/20\n",
      "cost: 279438.418918\n",
      "trying to converge 5/20\n",
      "cost: 295931.386301\n",
      "trying to converge 6/20\n",
      "cost: 152624.305212\n",
      "trying to converge 7/20\n",
      "cost: 352318.315347\n",
      "trying to converge 8/20\n",
      "cost: 320881.295529\n",
      "trying to converge 9/20\n",
      "cost: 320719.295107"
     ]
    }
   ],
   "source": [
    "import random, operator, subprocess\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "rdd = sc.textFile('data10k_6attr.csv') \\\n",
    "        .map(lambda line: line.split(',')) \\\n",
    "        .map(lambda elements: tuple([int(elements[i]) for i in range(len(elements))])) \\\n",
    "        .cache()\n",
    "\n",
    "k = 10\n",
    "dimension = 6\n",
    "headers = ['age', 'height', 'weight', 'blood_sugar_level', 'child', 'exercise_hours']\n",
    "max_cluster = rdd.count() / k\n",
    "min_cluster = rdd.count() / (2*k-1)\n",
    "loop_for_converge = 20\n",
    "different_combination = 30\n",
    "\n",
    "def dist(x, y):\n",
    "    return sum([abs(x[i]-y[i]) for i in range(dimension)])\n",
    "\n",
    "def get_nearest_centroid_idx(x, centroids):\n",
    "    dists = {}\n",
    "    for cluster in centroids:\n",
    "        dists[cluster] = dist(x, centroids[cluster])\n",
    "        \n",
    "    cluster = min(dists, key=dists.get)\n",
    "    return cluster\n",
    "\n",
    "def assign_to_cluster(pt, available_centroids):\n",
    "    nearest_centroid = get_nearest_centroid_idx(pt, available_centroids)\n",
    "    return (nearest_centroid, ([pt], [dist(pt, available_centroids[nearest_centroid])]))\n",
    "\n",
    "def calculate_pts_sum(pts):\n",
    "    pts_sum = [0 for _ in range(dimension)]\n",
    "    for pt in pts:\n",
    "        for i in range(dimension):\n",
    "            pts_sum[i] += pt[i]\n",
    "    return pts_sum\n",
    "\n",
    "def calculate_centroid(pts_sum, nb_pts):\n",
    "    nb_pts = float(nb_pts)\n",
    "    return [pts_sum[i]/nb_pts for i in range(dimension)]\n",
    "\n",
    "def popup_available_pts(pts, dists):\n",
    "    sorted_idx = sorted(range(len(dists)), key=lambda k: dists[k])\n",
    "    for i in sorted_idx[k:]:\n",
    "        yield pts[i]\n",
    "\n",
    "def keep_pts(pts, dists):\n",
    "    sorted_idx = sorted(range(len(dists)), key=lambda k: dists[k])\n",
    "    return [pts[i] for i in sorted_idx[:k]]\n",
    "\n",
    "def calculate_cost(pts, centroid):\n",
    "    cost = 0\n",
    "    for pt in pts:\n",
    "        cost += dist(pt, centroid)\n",
    "    return cost\n",
    "\n",
    "def is_converge(old_cens, new_cens):\n",
    "    diff = 0\n",
    "    old_sum = 0\n",
    "    for i in range(len(old_cens)):\n",
    "        old_cen = old_cens[i]\n",
    "        new_cen = new_cens[i]\n",
    "        for j in range(dimension):\n",
    "            diff += abs(new_cen[j] - old_cen[j])\n",
    "            old_sum += old_cen[j]\n",
    "    return abs(float(diff) / old_sum) < 0.000001\n",
    "\n",
    "def write_to_output(assignment, centroids):\n",
    "    tmp = assignment.flatMap(lambda (cluster, pts): [centroids[cluster] for _ in range(len(pts))])\n",
    "    sqlContext.createDataFrame(tmp, headers[:dimension]).save('output.txt', mode='overwrite')\n",
    "    \n",
    "min_cost_rdd = None\n",
    "min_cost = float('inf')\n",
    "for want_cluster in range(min_cluster, max_cluster+1):\n",
    "    print \"trying \" + str(want_cluster) + \" clusters\"\n",
    "    for combination_idx in range(different_combination): # try different combination of initial centroid\n",
    "        print \"trying combination \" + str(combination_idx) + \"/\" + str(different_combination)\n",
    "        \n",
    "        # convert centroids as a dictionary having index as key and centroid point as value\n",
    "        tmp_centroids = rdd.takeSample(False, want_cluster)\n",
    "        centroids = {}\n",
    "        for i in range(len(tmp_centroids)):\n",
    "            centroids[i] = tmp_centroids[i]\n",
    "        \n",
    "        for converge_idx in range(loop_for_converge): # try to converge\n",
    "            print \"trying to converge \" + str(converge_idx) + \"/\" + str(loop_for_converge)\n",
    "            \n",
    "            available_pts_rdd = rdd\n",
    "            available_centroids = centroids\n",
    "            assignment = None\n",
    "            \n",
    "            # ensure each cluster has at least k members (k-anonymity)\n",
    "            while(True):\n",
    "                cluster_rdd = sc.parallelize([(i, ([], [])) for i in available_centroids])\n",
    "                dist_to_cluster_rdd = available_pts_rdd.map(lambda pt: assign_to_cluster(pt, available_centroids)) \\\n",
    "                                         .reduceByKey(lambda (pt1, dist1), (pt2, dist2): (pt1+pt2,dist1+dist2)) \\\n",
    "                                         .cache()\n",
    "                \n",
    "                assignment_for_all_rdd = cluster_rdd.union(dist_to_cluster_rdd) \\\n",
    "                                           .reduceByKey(lambda (pts1, dists1), (pts2, dists2): (pts1+pts2, dists1+dists2)) \\\n",
    "                                            .cache()\n",
    "            \n",
    "                clusters_require_more_rdd = assignment_for_all_rdd.filter(lambda (cluster, (pts, dists)): len(dists) < k).cache()\n",
    "#                 print \"Require more: \" + str(clusters_require_more_rdd.count())\n",
    "#                 print \"Assignemtn: \" + str(assignment_for_all_rdd.collect())\n",
    "                if(clusters_require_more_rdd.count() > 1): # some cluster has less than k members\n",
    "                    completed_pts_rdd = assignment_for_all_rdd.map(lambda (cluster, (pts, dists)): (cluster, keep_pts(pts, dists)))\n",
    "                    assignment = completed_pts_rdd if assignment == None else assignment.union(completed_pts_rdd).cache()\n",
    "                    \n",
    "                    available_centroids = clusters_require_more_rdd.map(lambda (cluster, _): (cluster, available_centroids[cluster])) \\\n",
    "                                            .collectAsMap()\n",
    "\n",
    "                    available_pts_rdd = assignment_for_all_rdd.filter(lambda (cluster, (pts, dists)): len(dists) > k) \\\n",
    "                                            .flatMap(lambda (cluster, (pts, dists)): popup_available_pts(pts, dists))\n",
    "                else: # each cluster has at least k elements\n",
    "                    completed_pts_rdd = dist_to_cluster_rdd.map(lambda (cluster, (pts, dists)): (cluster, pts))\n",
    "                    assignment = completed_pts_rdd if assignment == None else assignment.union(completed_pts_rdd).cache()\n",
    "                    break\n",
    "            \n",
    "            assignment = assignment.reduceByKey(lambda x, y: x+y).cache() # final assignment rdd (cluster, pts)\n",
    "            # calculate new centroids based on all points inside a cluster\n",
    "            new_centroids = assignment.map(lambda (cluster, pts): (cluster, calculate_centroid(calculate_pts_sum(pts), len(pts))))\\\n",
    "                    .collectAsMap()\n",
    "#             print assignment.collect()\n",
    "\n",
    "            # update if it achieve smaller cost\n",
    "            cost = assignment.map(lambda (cluster, pts): calculate_cost(pts, new_centroids[cluster])).reduce(lambda x,y: x+y)\n",
    "            print \"cost: \" + str(cost)\n",
    "            if(cost < min_cost):\n",
    "                min_cost_rdd = assignment\n",
    "                min_cost = cost\n",
    "                write_to_output(min_cost_rdd, new_centroids)\n",
    "                \n",
    "            if(is_converge(centroids, new_centroids)):\n",
    "                break\n",
    "            centroids = new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
